{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "\n",
    "*Please fill out the relevant cells below according to the instructions. When done, save the notebook and export it to PDF, upload both the `ipynb` and the PDF file to Canvas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "\n",
    "*Group submission is highly encouraged. If you submit as part of group, list all group members here. Groups can comprise up to 4 students.*\n",
    "\n",
    "* Adam Applegate\n",
    "* Beatrix Brahms\n",
    "* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Central Limit Theorem (2pts)\n",
    "\n",
    "Use `scipy.stats` to draw $N$ samples from the uniform and the Cauchy distribution. Confirm whether the mean $\\mu$ of these samples (which is itself a RV) has a distribution $p(\\mu)$ that converges to a normal distribution when $N\\rightarrow\\infty$.\n",
    "\n",
    "A simple way of testing for normality of the distribution of means is the [68–95–99.7 rule](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule), i.e. you expect that there are only about 5% of the means (of a draw of $N$ samples) that deviate from $\\mathrm{mean}(\\mu)$ by more than $2 \\sqrt{\\mathrm{var}(\\mu)}$.\n",
    "\n",
    "Visualization can be helpful but is itself not a sufficient confirmation of normality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Hereditary Probabilities (3pts)\n",
    "\n",
    "The height of children is related to their parents: tall parents tend to have tall children. The same is true for small parents and small children. Over the course of just a few generations the standard deviation in children's heights should therefore get larger and larger. But this is not the case! The distribution of heights of children at fixed age is well described by a Normal and has been remarkable stable over hundreds of years (improvements in nutrition have led to height increases overall, but the standard deviation remains stable). Something does not add up! Francis Galton thought so, too, in a study in 1885. \n",
    "\n",
    "### Step 1 (1pt):\n",
    "\n",
    "Load the data he had assembled, from the file `Galton.txt`, into an array. Use `numpy.genfromtxt`, and make use of its arguments `names=True` and `dtype=None` to read in the column names from the header and choose the data type on its own as needed. You will get the columns\n",
    "\n",
    "* `Family`: The family that the child belongs to, labeled from 1 to 204 and 136A\n",
    "* `Father`: The father's height, in inches\n",
    "* `Mother`: The mother's height, in inches\n",
    "* `Gender`: The gender of the child, male (M) or female (F)\n",
    "* `Height`: The height of the child, in inches\n",
    "* `Kids`: The number of kids in the family of the child\n",
    "\n",
    "Make a visualization of the joint distribution of $X$, the parent's height (pick either father or mother), and $Y$, the children's height (pick either son or daughter).\n",
    "\n",
    "**Tip**: The `matplotlib.hist2d` is useful. Don't forget labels and units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (2pts)\n",
    "\n",
    "Select $X$ and $Y$ such that they have the same gender. Now compute the conditional distribution $p(y\\mid x > 71)$ (for fathers and sons) or $p(y\\mid x > 66)$ (for mothers and daughters). Plot their histograms and compute their means.\n",
    "\n",
    "Answer these three questions:\n",
    "\n",
    "* What do you find?\n",
    "* With the same data, can you think of a way to test whether tall parents are causally responsible for their children being less tall?\n",
    "* If there is no causal connection, what does that mean for conditioning on extreme events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Likelihood vs Prior (1pt)\n",
    "\n",
    "Leveraging again `scipy.stats`, reproduce the figure from [this](https://twitter.com/avehtari/status/1218896617346162688?s=20) tweet. To see it here, execute the next cell.\n",
    "\n",
    "In detail, choose [Student's t distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html#scipy.stats.t) with 4 degrees of freedom and the standard normal distribution for either the likelihood or the prior, but separate them by $\\delta x=10$. Compute and plot the posteriors.\n",
    "\n",
    "Which of these prior distributions is more robust to outliers in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": "true"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Here&#39;s an illustration of this...<br><br>...wait, where did that mule go? <a href=\"https://t.co/jR589tfZLE\">https://t.co/jR589tfZLE</a> <a href=\"https://t.co/c05luQePpQ\">pic.twitter.com/c05luQePpQ</a></p>&mdash; Aki Vehtari (@avehtari) <a href=\"https://twitter.com/avehtari/status/1218896617346162688?ref_src=twsrc%5Etfw\">January 19, 2020</a></blockquote>\n",
       "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<__main__.Tweet at 0x10fb531d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "class Tweet(object):\n",
    "    def __init__(self, s, embed_str=False):\n",
    "        if not embed_str:\n",
    "            # Use Twitter's oEmbed API\n",
    "            # https://dev.twitter.com/web/embedded-tweets\n",
    "            api = 'https://publish.twitter.com/oembed?url={}'.format(s)\n",
    "            response = requests.get(api)\n",
    "            self.text = response.json()[\"html\"]\n",
    "        else:\n",
    "            self.text = s\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return self.text\n",
    "\n",
    "Tweet(\"https://twitter.com/avehtari/status/1218896617346162688?s=20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Hubble was no Bayesian (4pts)\n",
    "\n",
    "...but you can be!\n",
    "\n",
    "In 1929, Edwin Hubble published a seminal [paper](http://www.pnas.org/content/pnas/15/3/168.full.pdf), in which he compared the radial velocity of astronomical objects (i.e. how fast these objects move towards or away from us) with their distance. The former can be done pretty precisely with spectroscopy, the latter is much more uncertain.\n",
    "\n",
    "He saw that the velocity increases with distance and speculated that this could be the sign of a cosmological expansion. This lead cosmologist to believe in the Big Bang theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0:\n",
    "\n",
    "Load the data from the file `hubble.txt` into an array with `numpy.genfromtxt`, and make again use of the arguments `names` and `dtype`. You should get 6 columns\n",
    "   * `CAT`, `NUMBER`:  These two combined give you the name of the galaxy.\n",
    "   * `R`: distance in Mpc\n",
    "   * `V`: radial velocity in km/s\n",
    "   * `RA`, `DEC`: equatorial coordinates of the galaxy\n",
    "   \n",
    "Make a scatter plot of $R$ vs $V$ (that means the independent variable is $V$). Don't forget labels and units..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 (1pt):\n",
    "\n",
    "Use linear regression to determine the MLE of the slope $b$ for the line $R=b V$. This is a linear model **with no intercept**. Print the MLE. Then, create a new version of the scatter plot by adding the MLE line.\n",
    "\n",
    "**Tip:** You don't need measurement uncertainties (there aren't any in Hubble's data) to determine the MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (2pts):\n",
    "\n",
    "The full Gaussian likelihood of the linear regression problem has a term for the intercept $a$, slope $b$, and *uncertainty* $\\sigma$ of $R$. We will assume that the uncertainties of all data points are identical. Adopt maximally uniformative priors for all of the parameters $\\theta=(a,b,\\sigma)$.\n",
    "\n",
    "Compute the log posterior on a reasonably fine grid of $(a,b,\\sigma)$, picking suitable limits for every parameter. Then marginalize out $\\sigma$ and plot the log posterior for the remaining parameters $(a,b)$.\n",
    "\n",
    "**Tip:** The function `scipy.special.logsumexp` is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 (1pt):\n",
    "\n",
    "Use the function `sample_2d` below to draw samples from the 2D array of the posterior of $(a,b)$. Create a final version of the scatter plot by adding the lines that correspond to these posterior draws.\n",
    "\n",
    "**Tip:** When plotting, set the transparency `alpha` to values < 1, so that multiple draws of the same parameter pair become visually more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://stackoverflow.com/questions/56017163\n",
    "def sample_2d(dist2d, n=50, replace=True):\n",
    "    \"\"\"\n",
    "    Given an array representing a 2D joint probability distribution p(x,y), return n index pairs (i_x, i_y) sampled\n",
    "    according to that distribution\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    shape = dist2d.shape\n",
    "    N = np.prod(shape)\n",
    "    \n",
    "    # generate the set of all x,y pairs represented by the pmf\n",
    "    pairs=np.indices(dimensions=shape).T # here are all of the x,y pairs \n",
    "\n",
    "    # make n random selections from the flattened pmf\n",
    "    inds = np.random.choice(np.arange(N), p=dist2d.reshape(N), size=n, replace=replace)\n",
    "\n",
    "    # inds is the set of n randomly chosen indicies into the flattened dist array...\n",
    "    # therefore the random x,y selections\n",
    "    # come from selecting the associated elements\n",
    "    # from the flattened pairs array\n",
    "    return pairs.reshape(-1,2)[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
